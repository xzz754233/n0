# src/research_events/research_events_graph.py
import asyncio
from typing import Literal

from langchain_tavily import TavilySearch
from langgraph.graph import END, START, StateGraph
from langgraph.types import Command
from langchain_core.runnables import RunnableConfig

from src.configuration import Configuration
from src.services.event_service import EventService
from src.state import ResearchState
from src.url_crawler.utils import url_crawl, chunk_text_by_tokens
from src.utils import get_langfuse_handler
from src.services.url_service import URLService


# 1. æœå°‹ç¯€é»
def search_node(state: ResearchState) -> Command[Literal["process_batch"]]:
    """Find relevant URLs using Tavily."""
    question = state.get("research_question")
    existing_urls = state.get("processed_urls", [])

    # ä½¿ç”¨ Tavily æœå°‹
    tavily = TavilySearch(
        max_results=3, include_answer=False, include_raw_content=False
    )
    search_results = tavily.invoke({"query": question})

    found_urls = [r["url"] for r in search_results.get("results", [])]

    # ç°¡å–®éæ¿¾æ‰å·²ç¶“çˆ¬éçš„
    new_urls = [url for url in found_urls if url not in existing_urls]

    print(f"ğŸ” Found {len(new_urls)} new URLs for: {question}")

    return Command(goto="process_batch", update={"target_urls": new_urls})


# 2. æ‰¹æ¬¡è™•ç†ç¯€é» (æ ¸å¿ƒå„ªåŒ–)
async def process_batch_node(
    state: ResearchState, config: RunnableConfig
) -> Command[Literal["__end__"]]:
    """
    Crawl and Extract events from ALL target URLs in parallel.
    This replaces the old loop-based merge logic.
    """
    urls = state.get("target_urls", [])
    question = state.get("research_question", "")

    if not urls:
        return Command(goto=END)

    print(f"ğŸš€ Batch processing {len(urls)} URLs...")

    # å®šç¾©å–®å€‹ URL çš„è™•ç†é‚è¼¯
    async def process_single_url(url):
        try:
            # A. çˆ¬å–
            content = await url_crawl(url)
            if not content:
                return []

            # B. åˆ†å¡Š
            chunks = await chunk_text_by_tokens(
                content, chunk_size=3000, overlap_size=100
            )

            # C. æå– (é€™ä¸€æ­¥æœƒèª¿ç”¨ EventService åšä¸¦ç™¼æå–)
            # é™åˆ¶æ¯å€‹ç¶²é æœ€å¤šçœ‹å‰ 3-4 å€‹ chunksï¼Œé¿å… token æµªè²»åœ¨ footer/å´é‚Šæ¬„
            limit_chunks = chunks[:4]
            events = await EventService.run_batch_extraction(
                limit_chunks, url, question, config
            )
            return events
        except Exception as e:
            print(f"âŒ Error processing {url}: {e}")
            return []

    # ä¸¦ç™¼åŸ·è¡Œæ‰€æœ‰ URL çš„è™•ç†
    results = await asyncio.gather(*[process_single_url(url) for url in urls])

    # å±•å¹³çµæœ
    all_new_events = [e for batch in results for e in batch]

    print(f"ğŸ“¦ Batch complete. Total raw events extracted: {len(all_new_events)}")

    # é€™è£¡åˆ©ç”¨ State çš„ operator.add è‡ªå‹•å°‡ all_new_events åŠ å…¥ gathered_events
    return Command(
        goto=END,
        update={
            "gathered_events": all_new_events,
            "processed_urls": urls,  # è¨˜éŒ„å·²è™•ç†
            "target_urls": [],  # æ¸…ç©ºå¾…è™•ç†éšŠåˆ—
        },
    )


# --- Graph Definition ---
workflow = StateGraph(ResearchState)
workflow.add_node("search_node", search_node)
workflow.add_node("process_batch", process_batch_node)

workflow.add_edge(START, "search_node")
# search_node ç›´æ¥æŒ‡æ´¾äº† goto="process_batch"ï¼Œé€™è£¡ä¸éœ€è¦ edgeï¼Œä½†ç‚ºäº†è¦–è¦ºåŒ–å¯ä»¥åŠ 
# workflow.add_edge("search_node", "process_batch")
workflow.add_edge("process_batch", END)

research_events_app = workflow.compile().with_config(
    {"callbacks": [get_langfuse_handler()]}
)
